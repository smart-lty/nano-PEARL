<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Quick Start • nano-PEARL</title>
  <meta name="color-scheme" content="dark light">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@600;700&family=Lora:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="./styles.css"/>
  <style>
    /* Page-scoped enhancements for Quick Start */
    html{ scroll-behavior: smooth }
    body.qs main{display:grid; grid-template-columns: 280px 1fr; gap:24px}
    /* Align content with hero text: shift content left, push ToC further left */
    body.qs .toc{position: sticky; top:84px; align-self:start; width:280px; margin-left: calc(-280px - 24px)}
    body.qs .kblocks{margin-left: calc(-280px - 24px); width: calc(100% + 280px + 24px)}
    @media (max-width: 980px){
      body.qs main{grid-template-columns: 1fr}
      body.qs .toc{position:static; margin-left:0; width:auto}
      body.qs .kblocks{margin-left:0; width:auto}
    }
    body.qs .toc .card{padding:18px; border-radius:14px}
    body.qs .toc h3{margin:0 0 8px; font-family:'Poppins', ui-sans-serif; font-size:14px; letter-spacing:.08em; text-transform:uppercase; color:var(--muted)}
    body.qs .toc a{display:block; padding:8px 10px; border-radius:10px; color:var(--muted); font-weight:600}
    body.qs .toc a:hover{background:#fff; color:var(--text); box-shadow:0 0 0 2px var(--ring)}

    body.qs .section.card{padding:24px}
    body.qs .section{ scroll-margin-top: 96px }
    body.qs .section h2{margin:0 0 6px; font-family:'Poppins', ui-sans-serif}
    body.qs .section .lead{color:var(--muted); margin:0 0 12px}
    body.qs .kblocks{display:grid; gap:18px}
    body.qs .kgrid{display:grid; gap:14px; grid-template-columns: repeat(auto-fit, minmax(220px,1fr))}

    /* Callouts */
    body.qs .callout{display:flex; gap:10px; padding:12px 14px; border-radius:12px; border:1px solid var(--line); background:#fff}
    body.qs .callout .ico{flex:0 0 18px; opacity:.9}
    body.qs .callout.info{border-color: rgba(106,155,204,.35); background: rgba(106,155,204,.06)}
    body.qs .callout.warn{border-color: rgba(217,119,87,.35); background: rgba(217,119,87,.06)}
    body.qs .callout.oom{border-color: rgba(111,66,193,.35); background: rgba(111,66,193,.06)}

    /* Code blocks */
    body.qs pre{
      margin:10px 0;
      background:#f5f5f5;
      border:1px solid #e5e5e5;
      border-left:3px solid #d9d9d9;
      border-radius:12px;
      padding:12px;
      overflow:auto;
      box-shadow: 0 3px 10px rgba(20,20,19,0.05);
    }
    body.qs code{
      font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, Liberation Mono, monospace;
      color: #1f2937;
    }

    /* Mini tiles */
    body.qs .tile{background:#fff; border:1px solid var(--line); border-radius:14px; padding:14px}
    body.qs .tile dt{font-size:11px; letter-spacing:.14em; text-transform:uppercase; color:var(--muted); font-family:'Poppins', ui-sans-serif; margin:0 0 6px}
    body.qs .tile dd{margin:0; font-weight:700; font-family:'Poppins', ui-sans-serif}

    /* Inline labels */
    body.qs .lbl{display:inline-block; padding:2px 8px; border-radius:999px; border:1px solid var(--line); color:var(--muted); font-size:11px; font-weight:700; font-family:'Poppins', ui-sans-serif}

    /* Python code highlight tokens */
    /* Conventional light theme token palette */
    .tok-k{ color: #005cc5; font-weight:700 }
    .tok-s{ color: #22863a }
    .tok-c{ color: #6a737d; font-style: italic }
    .tok-n{ color: #e36209 }
    .tok-fn{ color: #6f42c1; font-weight:700 }
    .tok-dec{ color: #d73a49 }

    /* Bash token palette */
    .tok-cmd{ color:#005cc5; font-weight:700 }
    .tok-opt{ color:#6f42c1 }
    .tok-var{ color:#e36209 }
    .tok-path{ color:#24292e; font-style: italic }
  </style>
</head>
<body class="qs">
  
    <div class="nav">
      <div class="nav-inner">
        <a class="logo" href="./index.html" aria-label="nano-PEARL Home">
          <svg viewBox="0 0 24 24" fill="none" aria-hidden="true">
            <path d="M4 12c5-9 11-9 16 0" stroke="url(#g1)" stroke-width="2.2" stroke-linecap="round"/>
            <path d="M4 12c5 9 11 9 16 0" stroke="url(#g2)" stroke-width="2.2" stroke-linecap="round"/>
            <defs>
              <linearGradient id="g1" x1="4" x2="20" y1="12" y2="12">
                <stop stop-color="#9AE6B4"/><stop offset="1" stop-color="#8AB4F8"/>
              </linearGradient>
              <linearGradient id="g2" x1="4" x2="20" y1="12" y2="12">
                <stop stop-color="#8AB4F8"/><stop offset="1" stop-color="#9AE6B4"/>
              </linearGradient>
            </defs>
          </svg>
          <span class="sig">nano-PEARL</span>
        </a>
        <nav class="nav-links" role="navigation" aria-label="Primary">
          <a href="./index.html" class="">Introduction</a>
          <a href="./quickstart.html" class="active">Quick Start</a>
          <a href="./benchmark.html" class="">BenchMark</a>
          <a href="./faqs.html" class="">FAQs</a>
        </nav>
      </div>
    </div>
    
  
    <section class="page-hero">
      <div class="container" style="padding:42px 24px 28px">
        <div class="kicker">Docs</div>
        <h1>Quick Start</h1>
        <p class="sub">Install, run, reproduce. Minimal steps for nano-PEARL.</p>
      </div>
    </section>
    <main class="container">
      <aside class="toc reveal">
        <div class="card">
          <h3>On this page</h3>
          <a href="#overview">Overview</a>
          <a href="#install">Installation</a>
          <a href="#basic">Basic Usage</a>
          <a href="#examples">Examples</a>
          <a href="#tips">Troubleshooting</a>
        </div>
      </aside>

      <section class="kblocks">
        <!-- Overview -->
        <div id="overview" class="section card reveal">
          <h2>Overview</h2>
          <p class="lead">Prerequisites and sensible defaults.</p>
          <div class="kgrid" style="margin-top:6px">
            <dl class="tile"><dt>Python</dt><dd>&ge; 3.12</dd></dl>
            <dl class="tile"><dt>PyTorch</dt><dd>&ge; 2.4</dd></dl>
            <dl class="tile"><dt>CUDA</dt><dd>12.x</dd></dl>
            <dl class="tile"><dt>Draft TP</dt><dd>1</dd></dl>
            <dl class="tile"><dt>Target TP</dt><dd>1–2</dd></dl>
            <dl class="tile"><dt>Dtype</dt><dd>bfloat16</dd></dl>
          </div>
          <p class="settings-footnote" style="margin-top:10px">Install tips included below for torch/flash-attn ordering.</p>
        </div>

        <!-- 1) Installation -->
        <div id="install" class="section card reveal">
          <h2>1. Installation</h2>
          <p class="lead">
            nano-PEARL builds on <a href="https://github.com/GeeeekExplorer/nano-vllm" target="_blank" rel="noreferrer">nano-vllm</a>.
            We recommend Python 3.12 and installing from source with <code>uv</code> (or <code>pip</code>).
          </p>
          <h3>1.1 Create environment</h3>
<pre><code class="language-bash">conda create -n nano-pearl python=3.12 -y
conda activate nano-pearl</code></pre>
          <h3>1.2 Install package</h3>
          <div class="kgrid">
            <div>
              <div class="lbl" style="margin-bottom:8px">From source (recommended)</div>
<pre><code class="language-bash">uv pip install -e .</code></pre>
            </div>
            <div>
              <div class="lbl" style="margin-bottom:8px">From GitHub</div>
<pre><code class="language-bash">pip install git+https://github.com/smart-lty/nano-PEARL.git</code></pre>
            </div>
          </div>
          <div class="callout info" style="margin-top:10px">
            <svg class="ico" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.8"><circle cx="12" cy="12" r="9"/><path d="M12 8h.01M11 11h2v5h-2"/></svg>
            <div>
              <strong>Install tips</strong>
              <div>If <code>flash-attn</code> build complains about missing <code>torch</code>, install a matching PyTorch first, then rerun. Prefer prebuilt wheels for your CUDA/PyTorch combo if available.</div>
            </div>
          </div>
        </div>

        <!-- 2) Basic Usage -->
        <div id="basic" class="section card reveal">
          <h2>2. Basic Usage</h2>
          <p class="lead">
            The API mirrors vLLM / nano-vllm. Provide both a <strong>draft</strong> model and a <strong>target</strong> model with their tensor-parallel sizes.
          </p>

          <h3 id="example-py" style="margin-top:8px">2.1 example.py</h3>
          <p>Minimal single-request example on 2 GPUs (TP: 1 + 1). Edit paths, then run:</p>
<pre><code class="language-bash">python example.py</code></pre>
          <p class="lead" style="margin:12px 0 6px"><em>Source (repo root: <code>example.py</code>) — quick single-turn Q&A:</em></p>
<pre><code class="language-python">import argparse
from nano_pearl import PEARLConfig, PEARLEngine, SamplingParams, logger

def main():
    draft_model_path = "/path/to/draft/model"
    target_model_path = "/path/to/target/model"
    
    config = PEARLConfig(draft_model_path, target_model_path, draft_tensor_parallel_size=1, target_tensor_parallel_size=1, gpu_memory_utilization=0.9)
    engine = PEARLEngine(config)
    
    prompt = "Explain quantum computing in simple terms"
    sampling_params = SamplingParams(temperature=0.0, max_tokens=256, ignore_eos=False)
    engine.add_request(prompt, sampling_params)
    
    output_text, num_tokens, num_acc_tokens, elapsed_time = engine.generate()
    logger.info(f"Completion:", color="yellow")
    logger.info(f"{output_text[0]}")
    logger.info(f"Tokens: {num_tokens[0]}, Time: {elapsed_time:.2f}s, Throughput: {num_tokens[0] / elapsed_time:.2f} tok/s, MAT: {sum(num_acc_tokens[0]) / len(num_acc_tokens[0])}")

if __name__ == "__main__":
    main()</code></pre>

          <h3 id="bench-py">2.2 bench.py</h3>
          <p>
            Batch benchmarking utility with warmup, optional AR baseline, and simple CLI.
            You can use the defaults prompts under <code>static/default_prompts.txt</code> or more benchmarks under <code>benchmark/data</code>.
          </p>
          <p><strong>Common invocation:</strong></p>
<pre><code class="language-bash">python bench.py \
  --draft-model "/path/to/draft" \
  --target-model "/path/to/target" \
  --draft-tp 1 --target-tp 2 \
  --max-tokens 200 --temperature 0 \
  --run-ar-benchmark -v</code></pre>
          <p><em>Notes:</em></p>
          <ul>
            <li><code>--ignore-eos</code>: generate until reaching <code>max_tokens</code> or EOS if omitted.</li>
            <li><code>-p/--custom-prompts</code>: provide one or more prompts inline; otherwise defaults are used.</li>
            <li>Warmup runs once with a short prompt to stabilize kernels and KV caches.</li>
          </ul>
          <p class="lead" style="margin:12px 0 6px"><em>Source (repo root: <code>bench.py</code>) — multi-batch/dataset benchmark:</em></p>
<pre><code class="language-python">import sys
import copy
import argparse
import os
from random import seed
import time
import torch
from nano_pearl import PEARLConfig, PEARLEngine, SamplingParams, logger

def parse_args():
    parser = argparse.ArgumentParser(description='PEARL Benchmark Tool')
    
    parser.add_argument('--draft-model', '-d', type=str, required=True,
                        help='Draft model path (required)')
    parser.add_argument('--target-model', '-t', type=str, required=True,
                        help='Target model path (required)')
    parser.add_argument('--draft-tp', type=int, default=1,
                        help='Draft model tensor parallel size (default: 1)')
    parser.add_argument('--target-tp', type=int, default=2,
                        help='Target model tensor parallel size (default: 2)')
    parser.add_argument('--gpu-memory-utilization', type=float, default=0.9,
                        help='GPU memory utilization (default: 0.9)')
    parser.add_argument('--temperature', '-temp', type=float, default=0.0,
                        help='Sampling temperature (default: 0.0)')
    parser.add_argument('--max-tokens', type=int, default=200,
                        help='Maximum tokens to generate (default: 200)')
    parser.add_argument('--ignore-eos', '-noeos', action='store_true',
                        help='Ignore EOS token (default: False)')
    parser.add_argument('--run-ar-benchmark', '-ar', action='store_true',
                        help='Run AR (Autoregressive) benchmark (default: False)')
    parser.add_argument('--custom-prompts', '-p', type=str, nargs='+',
                        help='Custom prompts for benchmark')
    parser.add_argument('--seed', type=int, default=0,
                        help='Random seed (default: 0)')
    parser.add_argument('--verbose', '-v', action='store_true',
                        help='Verbose output (default: False)')
    
    return parser.parse_args()

def get_default_prompts():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    prompts_file = os.path.join(script_dir, 'static', 'default_prompts.txt')
    with open(prompts_file, 'r', encoding='utf-8') as f:
        prompts = [line.strip() for line in f.readlines() if line.strip()]
    return prompts

if __name__ == "__main__":
    args = parse_args()
    
    seed(args.seed)
    draft_model_path = args.draft_model
    target_model_path = args.target_model
    config = PEARLConfig(
        draft_model_path, 
        target_model_path, 
        draft_tensor_parallel_size=args.draft_tp, 
        target_tensor_parallel_size=args.target_tp, 
        gpu_memory_utilization=args.gpu_memory_utilization
    )
    engine = PEARLEngine(config)

    # warmup
    prompt = "Benchmark:"
    sampling_params = SamplingParams(temperature=0, ignore_eos=False, max_tokens=512)
    engine.add_request(prompt, sampling_params)
    output_text, num_tokens, num_acc_tokens, elapsed_time = engine.generate()
    MAT = [sum(n) / len(n) for n in num_acc_tokens]
    logger.info(f"[Warmup] Total: {sum(num_tokens)}tok, Time: {elapsed_time:.2f}s, Throughput: {sum(num_tokens) / elapsed_time:.2f}tok/s, MAT: {MAT}")

    prompts = args.custom_prompts if args.custom_prompts else get_default_prompts()
    sampling_params = SamplingParams(
        temperature=args.temperature, 
        ignore_eos=args.ignore_eos, 
        max_tokens=args.max_tokens
    )
    for prompt in prompts:
        engine.add_request(prompt, copy.deepcopy(sampling_params))

    output_text, num_tokens, num_acc_tokens, elapsed_time = engine.bench_generate(num_pearl_steps=100)
    MAT = [sum(n) / len(n) for n in num_acc_tokens]

    if args.verbose:
        for prompt, output_text in zip(prompts, output_text):
            logger.info(f"Prompt: \n{prompt}", color="yellow")
            logger.info(f"Completion: \n{output_text}")

    PEARL_throughput = sum(num_tokens) / elapsed_time
    logger.info(f"num_tokens: {num_tokens}, MAT: {MAT}")
    logger.info(f"[PEARL Generate] Batch Size: {len(prompts)} Total: {sum(num_tokens)}tok, Time: {elapsed_time:.2f}s, Throughput: {sum(num_tokens) / elapsed_time:.2f}tok/s, MAT: {MAT}")

    if args.run_ar_benchmark:
        sampling_params = SamplingParams(
            temperature=args.temperature, 
            ignore_eos=args.ignore_eos, 
            max_tokens=args.max_tokens
        )
        for prompt in prompts:
            engine.add_request(prompt, copy.deepcopy(sampling_params))

        output_text, num_tokens, _, elapsed_time = engine.AR_generate()
        if args.verbose:
            for prompt, output_text in zip(prompts, output_text):
                logger.info(f"Prompt: \n{prompt}", color="yellow")
                logger.info(f"Completion: \n{output_text}")
        AR_throughput = sum(num_tokens) / elapsed_time
        logger.info(f"[AR Generate] Batch Size: {len(prompts)} Total: {sum(num_tokens)}tok, Time: {elapsed_time:.2f}s, Throughput: {sum(num_tokens) / elapsed_time:.2f}tok/s")
        logger.info(f"PEARL Speedup: {PEARL_throughput / AR_throughput:.2f}x")</code></pre>

          <div class="callout info" style="margin-top:10px">
            <svg class="ico" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.8"><circle cx="12" cy="12" r="9"/><path d="M7 12h10M12 7v10"/></svg>
            <div>
              <strong>Tip</strong>
              <div>Choose <code>--target-tp</code> according to your available GPUs and model size; nano-PEARL currently supports static TP per model group.</div>
            </div>
          </div>
        </div>

        <!-- 3) Examples (Llama 3) -->
        <div id="examples" class="section card reveal">
          <h2>3. Examples — Llama&nbsp;3 series</h2>
          <p>
            Below are example pairings for Llama 3/3.1/3.2 checkpoints. Replace paths with your local
            model locations or Hugging Face cache paths. Ensure tokenizer/arch compatibility between
            draft and target.
          </p>
          <h3>3.1 Draft 8B → Target 70B</h3>
<pre><code class="language-bash">python bench.py \
  --draft-model "/models/meta-llama/Meta-Llama-3-8B-Instruct" \
  --target-model "/models/meta-llama/Meta-Llama-3-70B-Instruct" \
  --draft-tp 1 --target-tp 2 \
  --max-tokens 200 --temperature 0 \
  --run-ar-benchmark -v</code></pre>
          <h3>3.2 Draft 3B (Llama&nbsp;3.2) → Target 8B</h3>
<pre><code class="language-bash">python bench.py \
  --draft-model "/models/meta-llama/Llama-3.2-3B-Instruct" \
  --target-model "/models/meta-llama/Meta-Llama-3-8B-Instruct" \
  --draft-tp 1 --target-tp 1 \
  --max-tokens 200 --temperature 0 -v</code></pre>
          <p>
            For a quick single-request smoke test with Llama 3, you can also run <code>example.py</code>
            after setting the same draft/target paths:
          </p>
<pre><code class="language-bash">python example.py</code></pre>
        </div>

        <!-- Troubleshooting -->
        <div id="tips" class="section card reveal">
          <h2>4. Troubleshooting</h2>
          <div class="kgrid">
            <div class="callout warn">
              <svg class="ico" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.8"><path d="M12 9v4"/><path d="M12 17h.01"/><path d="M10.29 3.86 1.82 18a2 2 0 0 0 1.71 3h16.94a2 2 0 0 0 1.71-3L13.71 3.86a2 2 0 0 0-3.42 0Z"/></svg>
              <div>
                <strong>flash-attn build fails</strong>
                <div>Install a torch build matching your CUDA first, then reinstall. Prefer prebuilt wheels when available.</div>
              </div>
            </div>
            <div class="callout info">
              <svg class="ico" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.8"><circle cx="12" cy="12" r="9"/><path d="M12 8h.01M11 11h2v5h-2"/></svg>
              <div>
                <strong>Slow first run</strong>
                <div>Run a short warmup prompt to stabilize kernels and KV caches before measuring throughput.</div>
              </div>
            </div>
          </div>
          <!-- OOM block as a full-width row below -->
          <div class="callout oom" style="margin-top:12px">
            <svg class="ico" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.8"><rect x="5" y="6" width="14" height="12" rx="2"/><path d="M8 10h8M8 14h5"/></svg>
            <div>
              <strong>Out-of-Memory (OOM)</strong>
              <div style="margin-top:6px"><em>Memory & Batching Parameters (pearl_config.py, pearl_engine.py)</em></div>
              <ul style="margin:8px 0 0 18px">
                <li><code>gpu_memory_utilization: float</code> — fraction of GPU memory reserved for KV cache. <strong>Key knob</strong>. Recommendation: <code>0.9</code>.</li>
              </ul>
              <div class="help" style="margin:6px 0 0">Note: tune the following based on GPU memory and deployment scenario.</div>
              <ul style="margin:8px 0 0 18px">
                <li><code>max_num_batched_tokens: int</code> — max tokens in a single batch (<code>batch_size * seq_len</code>). Recommendation: <code>16384</code> on 80GB (H100/A100); <code>8192</code> on 40GB.</li>
                <li><code>max_num_seqs: int</code> — max concurrent sequences. Recommendation: <code>512</code> (80GB); <code>128</code>–<code>256</code> (40GB).
                </li>
                <li><code>max_model_len: int</code> — maximum context length (prompt + generated tokens).</li>
                <li><code>set_gamma_batch_size: list[int]</code> — batch sizes for auto-tuning when <code>gamma = -1</code>. Recommendation: default <code>[1,2,4,8,16,32,64]</code>; reduce (e.g., <code>[1,2,4,8]</code>) on constrained GPUs.</li>
              </ul>
              <div class="help" style="margin-top:6px">Practical tip: if OOM occurs, first lower <code>max_num_batched_tokens</code> or <code>max_num_seqs</code>, then adjust <code>gpu_memory_utilization</code> slightly (e.g., 0.85).</div>
            </div>
          </div>
        </div>

      </section>
    </main>
    
  <footer class="footer">
    <div class="container">
      nano-PEARL • © 2025
    </div>
  </footer>
  <script src="./site.js"></script>
</body>
</html>
